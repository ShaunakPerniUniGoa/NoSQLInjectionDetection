Model,Linear SVM,Poly SVM,RBF SVM,Logistic Regression,Random Forest,Gradient Boosting,KNN,Decsion Tree,Bagging,Adaboost,Sigmoid SVM,Navie Bayse
AccuracyA1,0.7555555555555555,0.7777777777777778,0.8,0.7555555555555555,0.7555555555555555,0.7777777777777778,0.6222222222222222,0.7333333333333333,0.7111111111111111,0.7555555555555555,0.6888888888888889,0.7333333333333333
PrecisionA1,0.6956521739130435,0.7777777777777778,0.7894736842105263,0.7368421052631579,0.6956521739130435,0.75,0.5714285714285714,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.6153846153846154,0.6666666666666666
RecallA1,0.8,0.7,0.75,0.7,0.8,0.75,0.6,0.8,0.7,0.75,0.8,0.8
F1A1,0.7441860465116279,0.7368421052631577,0.7692307692307692,0.717948717948718,0.7441860465116279,0.75,0.5853658536585366,0.7272727272727272,0.6829268292682926,0.7317073170731706,0.6956521739130435,0.7272727272727272
AccuracyA2,0.5111111111111111,0.4666666666666667,0.5777777777777777,0.5333333333333333,0.6444444444444445,0.6666666666666666,0.5555555555555556,0.6222222222222222,0.6666666666666666,0.6666666666666666,0.5777777777777777,0.6444444444444445
PrecisionA2,0.4583333333333333,0.4166666666666667,0.5263157894736842,0.4761904761904761,0.625,0.6470588235294118,0.5,0.6,0.631578947368421,0.6666666666666666,0.5263157894736842,0.6428571428571429
RecallA2,0.55,0.5,0.5,0.5,0.5,0.55,0.35,0.45,0.6,0.5,0.5,0.45
F1A2,0.5,0.4545454545454545,0.5128205128205129,0.4878048780487805,0.5555555555555556,0.5945945945945946,0.4117647058823529,0.5142857142857143,0.6153846153846154,0.5714285714285715,0.5128205128205129,0.5294117647058824
AccuracyB1,0.7333333333333333,0.7333333333333333,0.8222222222222222,0.7111111111111111,0.8,0.7777777777777778,0.7111111111111111,0.7333333333333333,0.7555555555555555,0.7333333333333333,0.6888888888888889,0.6444444444444445
PrecisionB1,0.72,0.7857142857142857,0.8,0.6842105263157895,0.7619047619047619,0.75,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.6818181818181818,0.6153846153846154,0.5769230769230769
RecallB1,0.782608695652174,0.55,0.8,0.65,0.8,0.75,0.7,0.8,0.75,0.75,0.8,0.75
F1B1,0.7499999999999999,0.6470588235294117,0.8000000000000002,0.6666666666666667,0.7804878048780488,0.75,0.6829268292682926,0.7272727272727272,0.7317073170731706,0.7142857142857143,0.6956521739130435,0.6521739130434783
AccuracyB2,0.7333333333333333,0.6,0.6222222222222222,0.6222222222222222,0.6222222222222222,0.6666666666666666,0.6222222222222222,0.6222222222222222,0.6222222222222222,0.6666666666666666,0.5777777777777777,0.6222222222222222
PrecisionB2,0.7619047619047619,0.55,0.5714285714285714,0.5789473684210527,0.5882352941176471,0.6470588235294118,0.6153846153846154,0.5882352941176471,0.5882352941176471,0.6666666666666666,0.5294117647058824,0.8
RecallB2,0.6956521739130435,0.55,0.6,0.55,0.5,0.55,0.4,0.5,0.5,0.5,0.45,0.2
F1B2,0.7272727272727272,0.55,0.5853658536585366,0.5641025641025641,0.5405405405405405,0.5945945945945946,0.4848484848484849,0.5405405405405405,0.5405405405405405,0.5714285714285715,0.4864864864864864,0.32
AccuracyC,0.6666666666666666,0.6,0.6,0.5777777777777777,0.6,0.6,0.5777777777777777,0.6,0.6,0.5777777777777777,0.6,0.5777777777777777
PrecisionC,0.5,0.55,0.55,0.52,0.55,0.55,0.5263157894736842,0.55,0.625,0.52,0.55,0.52
RecallC,0.6,0.55,0.55,0.65,0.55,0.55,0.5,0.55,0.25,0.65,0.55,0.65
F1C,0.5454545454545454,0.55,0.55,0.5777777777777778,0.55,0.55,0.5128205128205129,0.55,0.3571428571428571,0.5777777777777778,0.55,0.5777777777777778
